{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7affbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ae15eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::321097665711:role/service-role/AmazonSageMaker-ExecutionRole-20230629T130572\n"
     ]
    }
   ],
   "source": [
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto3.session.Session(region_name='eu-north-1'))\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5a75f4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, file_path, batch_size=32, shuffle=True):\n",
    "        from s3fs.core import S3FileSystem\n",
    "        self.s3 = S3FileSystem()\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.file_types = [\"train_frames_x\", \"train_frames_y\", \"train_frames_embed\", \"train_frames_decoder_input\"]\n",
    "        self.indices = []\n",
    "        for i in range(1, 2):\n",
    "            for j in range(1, 2):\n",
    "                self.indices += [f\"_{i}_{j}\"]\n",
    "        self.on_epoch_end()\n",
    "        self.total_samples = 0\n",
    "        self.num_samples_per_index = {}\n",
    "        for index in self.indices:\n",
    "            data = np.load(self.s3.open(os.path.join(self.file_path, self.file_types[3] + index + \".pkl\")), allow_pickle=True)\n",
    "            self.total_samples += data.shape[0]\n",
    "            self.num_samples_per_index[index] = data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.total_samples/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        lower_idx = index * self.batch_size\n",
    "        upper_idx = (index+1) * self.batch_size - 1\n",
    "        \n",
    "        start = 0\n",
    "        end = 0\n",
    "        begin = 0\n",
    "        finish = 0\n",
    "        starting_idx = \"\"\n",
    "        ending_idx = \"\"\n",
    "        \n",
    "        for idx in self.indices:\n",
    "            end = start + self.num_samples_per_index[idx]\n",
    "            if lower_idx < end and lower_idx >= start:\n",
    "                starting_idx = idx\n",
    "                begin = lower_idx-start\n",
    "            if upper_idx < end and upper_idx >= start:\n",
    "                ending_idx = idx\n",
    "                finish = upper_idx-start\n",
    "                break\n",
    "            start = end\n",
    "        \n",
    "        train_data_x, train_data_embed, train_data_decoder_input, train_data_y = [], [], [], []\n",
    "        started = False\n",
    "        for idx in self.indices:\n",
    "            if not started and idx != starting_idx:\n",
    "                continue\n",
    "            if idx == starting_idx:\n",
    "                if idx == ending_idx:\n",
    "                    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    break\n",
    "                else:\n",
    "                    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [self.s3.open(np.load(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "            elif idx != ending_idx:\n",
    "                train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "            elif idx == ending_idx:\n",
    "                train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "        \n",
    "        train_data_x, train_data_y, train_data_embed, train_data_decoder_input = np.concatenate(train_data_x), np.concatenate(train_data_y), np.concatenate(train_data_embed), np.concatenate(train_data_decoder_input)\n",
    "        \n",
    "        return [train_data_x[:, :, 1:], train_data_embed[:, 1, 1:], train_data_decoder_input[:, :, 2:]], train_data_y[:, :, 1:]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "def load_data(train_data_path):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    file_types = [\"train_frames_x_1\", \"train_frames_y_1\", \"train_frames_embed_1\", \"train_frames_decoder_input_1\"]\n",
    "    for file_type in file_types:\n",
    "        \n",
    "        data = []\n",
    "        for file in sorted(glob.glob(os.path.join(train_data_path, file_type+\"*\"))):\n",
    "            data.append(np.load(str(file), allow_pickle=True))\n",
    "        data = np.array(data)\n",
    "        train_data[file_type] = data\n",
    "        \n",
    "        data = []\n",
    "        for file in sorted(glob.glob(os.path.join(\"test\" + train_data_path[5:], file_type+\"*\"))):\n",
    "            data.append(np.load(str(file), allow_pickle=True))\n",
    "        data = np.array(data)\n",
    "        test_data[file_type] = data\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "def define_model(num_embedding_features, num_historical_features, historical_data_window, future_prediction_window):\n",
    "    \n",
    "    encoder_input = keras.Input(shape=(historical_data_window, num_historical_features))\n",
    "    encoder_lstm1 = layers.LSTM(32, return_sequences=True)(encoder_input)\n",
    "    encoder_lstm1 = layers.ReLU()(encoder_lstm1)\n",
    "    batch_norm1 = layers.BatchNormalization()(encoder_lstm1)\n",
    "    encoder_lstm2 = layers.LSTM(64, return_sequences=True)(batch_norm1)\n",
    "    encoder_lstm2 = layers.ReLU()(encoder_lstm2)\n",
    "    batch_norm2 = layers.BatchNormalization()(encoder_lstm2)\n",
    "    encoder_output = layers.LSTM(32)(batch_norm2)\n",
    "    encoder_output = layers.ReLU()(encoder_output)\n",
    "\n",
    "    embedding_input = keras.Input(shape=(num_embedding_features, ))\n",
    "    embedding_layer1 = layers.Dense(32)(embedding_input)\n",
    "    embedding_layer1 = layers.ReLU()(embedding_layer1)\n",
    "    batch_norm3 = layers.BatchNormalization()(embedding_layer1)\n",
    "    embedding_layer2 = layers.Dense(64)(batch_norm3)\n",
    "    embedding_layer2 = layers.ReLU()(embedding_layer2)\n",
    "    batch_norm4 = layers.BatchNormalization()(embedding_layer2)\n",
    "    embedding_output = layers.Dense(32)(batch_norm4)\n",
    "    embedding_output = layers.ReLU()(embedding_output)\n",
    "\n",
    "    embedding_encoder_concatenate = layers.Concatenate()([encoder_output, embedding_output])\n",
    "    embedding_encoder_concatenate = layers.Dense(32)(embedding_encoder_concatenate)\n",
    "    embedding_encoder_concatenate = layers.ReLU()(embedding_encoder_concatenate)\n",
    "    future_cpc = keras.Input(shape=(FUTURE_PREDICTION_WINDOW, ))\n",
    "\n",
    "    decoder_input = layers.RepeatVector(future_prediction_window)(embedding_encoder_concatenate)\n",
    "    future_cpc = layers.Reshape((-1, 1))(future_cpc)\n",
    "    decoder_input = layers.Concatenate()([decoder_input, future_cpc])\n",
    "    decoder_lstm1 = layers.LSTM(32, return_sequences=True)(decoder_input)\n",
    "    decoder_lstm1 = layers.ReLU()(decoder_lstm1)\n",
    "    batch_norm5 = layers.BatchNormalization()(decoder_lstm1)\n",
    "    decoder_lstm2 = layers.LSTM(16, return_sequences=True)(batch_norm5)\n",
    "    decoder_lstm2 = layers.ReLU()(decoder_lstm2)\n",
    "    batch_norm6 = layers.BatchNormalization()(decoder_lstm2)\n",
    "    decoder_output = layers.LSTM(2, return_sequences=True)(batch_norm6)\n",
    "    decoder_output_fin = layers.ReLU()(decoder_output)\n",
    "\n",
    "    # Create the model\n",
    "    ED_lstm_model = tf.keras.Model(inputs=[encoder_input, embedding_input, future_cpc], outputs=decoder_output_fin)\n",
    "    ED_lstm_model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "    \n",
    "    return ED_lstm_model\n",
    "\n",
    "def prepare_data(train_data, test_data):\n",
    "    \n",
    "    for key, value in train_data.items():\n",
    "        train_data[key] = np.nan_to_num(np.array(value).astype(np.float32)).astype(np.float32)\n",
    "        \n",
    "    for key, value in test_data.items():\n",
    "        test_data[key] = np.nan_to_num(np.array(value).astype(np.float32)).astype(np.float32)\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "def train_model(epochs, batchsize, model_path, model, train_generator, valid_generator):\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "                         generator = train_generator,\n",
    "                         validation_data = valid_generator,\n",
    "                         use_multiprocessing = True\n",
    "                         workers=2,\n",
    "                         verbose=1,\n",
    "                         callbacks=[\n",
    "                            tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                min_delta=0,\n",
    "                                patience=10,\n",
    "                                verbose=1,\n",
    "                                mode='min'\n",
    "                            ),\n",
    "                            tf.keras.callbacks.ModelCheckpoint(\n",
    "                                model_path,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                mode='min',\n",
    "                                verbose=1\n",
    "                            )\n",
    "                         ],\n",
    "                     )\n",
    "    return history\n",
    "                         \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    path = \"/opt/ml/processing/input/data/\"\n",
    "    model_path = \"/opt/ml/processing/output/trained_model.ckpt\"\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    train_data_gen = DataGenerator(\"s3://training-data-lstm/processed_data_1/\", 32, True, \"train\")\n",
    "    val_data_gen = DataGenerator(\"s3://training-data-lstm/processed_data_1/\", 32, True, \"test\")\n",
    "    \n",
    "    num_embedding_features = 14\n",
    "    num_historical_features = 14\n",
    "    historical_data_window = 14\n",
    "    future_prediction_window = 3\n",
    "                         \n",
    "    model = define_model(num_embedding_features, num_historical_features, historical_data_window, future_prediction_window)\n",
    "    history = train_model(epochs, batch_size, model_path, model, train_data_gen, val_data_gen)\n",
    "    \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea803efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, file_path, batch_size=32, shuffle=True, train_or_valid=\"train\"):\n",
    "        from s3fs.core import S3FileSystem\n",
    "        self.s3 = S3FileSystem()\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.file_types = [f\"{train_or_valid}_frames_x\", f\"{train_or_valid}_frames_y\", f\"{train_or_valid}_frames_embed\", f\"{train_or_valid}_frames_decoder_input\"]\n",
    "        self.indices = []\n",
    "        for i in range(1, 2):\n",
    "            for j in range(1, 2):\n",
    "                self.indices += [f\"_{i}_{j}\"]\n",
    "        self.on_epoch_end()\n",
    "        self.total_samples = 0\n",
    "        self.num_samples_per_index = {}\n",
    "        for index in self.indices:\n",
    "            data = np.load(self.s3.open(os.path.join(self.file_path, self.file_types[3] + index + \".pkl\")), allow_pickle=True)\n",
    "            self.total_samples += data.shape[0]\n",
    "            self.num_samples_per_index[index] = data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.total_samples/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        lower_idx = index * self.batch_size\n",
    "        upper_idx = (index+1) * self.batch_size - 1\n",
    "        \n",
    "        start = 0\n",
    "        end = 0\n",
    "        begin = 0\n",
    "        finish = 0\n",
    "        starting_idx = \"\"\n",
    "        ending_idx = \"\"\n",
    "        \n",
    "        for idx in self.indices:\n",
    "            end = start + self.num_samples_per_index[idx]\n",
    "            if lower_idx < end and lower_idx >= start:\n",
    "                starting_idx = idx\n",
    "                begin = lower_idx-start\n",
    "            if upper_idx < end and upper_idx >= start:\n",
    "                ending_idx = idx\n",
    "                finish = upper_idx-start\n",
    "                break\n",
    "            start = end\n",
    "        \n",
    "        train_data_x, train_data_embed, train_data_decoder_input, train_data_y = [], [], [], []\n",
    "        started = False\n",
    "        for idx in self.indices:\n",
    "            if not started and idx != starting_idx:\n",
    "                continue\n",
    "            if idx == starting_idx:\n",
    "                if idx == ending_idx:\n",
    "                    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    break\n",
    "                else:\n",
    "                    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [self.s3.open(np.load(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "            elif idx != ending_idx:\n",
    "                train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "            elif idx == ending_idx:\n",
    "                train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "        \n",
    "        train_data_x, train_data_y, train_data_embed, train_data_decoder_input = np.concatenate(train_data_x), np.concatenate(train_data_y), np.concatenate(train_data_embed), np.concatenate(train_data_decoder_input)\n",
    "        \n",
    "        return [np.nan_to_num(train_data_x[:, :, 1:]).astype(np.float32), np.nan_to_num(np.squeeze(train_data_embed[:, :, 1:])).astype(np.float32), np.nan_to_num(train_data_decoder_input[:, :, 2:]).astype(np.float32)], np.nan_to_num(train_data_y[:, :, 1:]).astype(np.float32)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "def load_data(train_data_path):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    file_types = [\"train_frames_x_1\", \"train_frames_y_1\", \"train_frames_embed_1\", \"train_frames_decoder_input_1\"]\n",
    "    for file_type in file_types:\n",
    "        \n",
    "        data = []\n",
    "        for file in sorted(glob.glob(os.path.join(train_data_path, file_type+\"*\"))):\n",
    "            data.append(np.load(str(file), allow_pickle=True))\n",
    "        data = np.array(data)\n",
    "        train_data[file_type] = data\n",
    "        \n",
    "        data = []\n",
    "        for file in sorted(glob.glob(os.path.join(\"test\" + train_data_path[5:], file_type+\"*\"))):\n",
    "            data.append(np.load(str(file), allow_pickle=True))\n",
    "        data = np.array(data)\n",
    "        test_data[file_type] = data\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "def define_model(num_embedding_features, num_historical_features, historical_data_window, future_prediction_window):\n",
    "    \n",
    "    encoder_input = keras.Input(shape=(historical_data_window, num_historical_features))\n",
    "    encoder_lstm1 = layers.LSTM(32, return_sequences=True)(encoder_input)\n",
    "    encoder_lstm1 = layers.ReLU()(encoder_lstm1)\n",
    "    batch_norm1 = layers.BatchNormalization()(encoder_lstm1)\n",
    "    encoder_lstm2 = layers.LSTM(64, return_sequences=True)(batch_norm1)\n",
    "    encoder_lstm2 = layers.ReLU()(encoder_lstm2)\n",
    "    batch_norm2 = layers.BatchNormalization()(encoder_lstm2)\n",
    "    encoder_output = layers.LSTM(32)(batch_norm2)\n",
    "    encoder_output = layers.ReLU()(encoder_output)\n",
    "\n",
    "    embedding_input = keras.Input(shape=(num_embedding_features, ))\n",
    "    embedding_layer1 = layers.Dense(32)(embedding_input)\n",
    "    embedding_layer1 = layers.ReLU()(embedding_layer1)\n",
    "    batch_norm3 = layers.BatchNormalization()(embedding_layer1)\n",
    "    embedding_layer2 = layers.Dense(64)(batch_norm3)\n",
    "    embedding_layer2 = layers.ReLU()(embedding_layer2)\n",
    "    batch_norm4 = layers.BatchNormalization()(embedding_layer2)\n",
    "    embedding_output = layers.Dense(32)(batch_norm4)\n",
    "    embedding_output = layers.ReLU()(embedding_output)\n",
    "\n",
    "    embedding_encoder_concatenate = layers.Concatenate()([encoder_output, embedding_output])\n",
    "    embedding_encoder_concatenate = layers.Dense(32)(embedding_encoder_concatenate)\n",
    "    embedding_encoder_concatenate = layers.ReLU()(embedding_encoder_concatenate)\n",
    "    future_cpc = keras.Input(shape=(future_prediction_window, ))\n",
    "\n",
    "    decoder_input = layers.RepeatVector(future_prediction_window)(embedding_encoder_concatenate)\n",
    "    future_cpc = layers.Reshape((-1, 1))(future_cpc)\n",
    "    decoder_input = layers.Concatenate()([decoder_input, future_cpc])\n",
    "    decoder_lstm1 = layers.LSTM(32, return_sequences=True)(decoder_input)\n",
    "    decoder_lstm1 = layers.ReLU()(decoder_lstm1)\n",
    "    batch_norm5 = layers.BatchNormalization()(decoder_lstm1)\n",
    "    decoder_lstm2 = layers.LSTM(16, return_sequences=True)(batch_norm5)\n",
    "    decoder_lstm2 = layers.ReLU()(decoder_lstm2)\n",
    "    batch_norm6 = layers.BatchNormalization()(decoder_lstm2)\n",
    "    decoder_output = layers.LSTM(2, return_sequences=True)(batch_norm6)\n",
    "    decoder_output_fin = layers.ReLU()(decoder_output)\n",
    "\n",
    "    # Create the model\n",
    "    ED_lstm_model = tf.keras.Model(inputs=[encoder_input, embedding_input, future_cpc], outputs=decoder_output_fin)\n",
    "    ED_lstm_model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "    \n",
    "    return ED_lstm_model\n",
    "\n",
    "def prepare_data(train_data, test_data):\n",
    "    \n",
    "    for key, value in train_data.items():\n",
    "        train_data[key] = np.nan_to_num(np.array(value).astype(np.float32)).astype(np.float32)\n",
    "        \n",
    "    for key, value in test_data.items():\n",
    "        test_data[key] = np.nan_to_num(np.array(value).astype(np.float32)).astype(np.float32)\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "def train_model(epochs, batchsize, model_path, model, train_generator, valid_generator):\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "                         generator = train_generator,\n",
    "                         validation_data = valid_generator,\n",
    "                         use_multiprocessing = True,\n",
    "                         workers=2,\n",
    "                         verbose=1,\n",
    "                         callbacks=[\n",
    "                            tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                min_delta=0,\n",
    "                                patience=10,\n",
    "                                verbose=1,\n",
    "                                mode='min'\n",
    "                            ),\n",
    "                            tf.keras.callbacks.ModelCheckpoint(\n",
    "                                model_path,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                mode='min',\n",
    "                                verbose=1\n",
    "                            )\n",
    "                         ],\n",
    "                     )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95caa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5/10409 [..............................] - ETA: 68:05:59 - loss: 6.3599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/utils/data_utils.py\", line 647, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/tmp/ipykernel_11396/3441579348.py\", line 72, in __getitem__\n",
      "    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 441, in load\n",
      "    return pickle.load(fid, **pickle_kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/spec.py\", line 1748, in read\n",
      "    out = self.cache._fetch(self.loc, self.loc + length)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/caching.py\", line 387, in _fetch\n",
      "    new = self.fetcher(start, self.start)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/s3fs/core.py\", line 1200, in _fetch_range\n",
      "    return _fetch_range(self.fs.s3, self.bucket, self.key, self.version_id, start, end, req_kw=self.req_kw)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/s3fs/core.py\", line 1337, in _fetch_range\n",
      "    return resp['Body'].read()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/response.py\", line 99, in read\n",
      "    chunk = self._raw_stream.read(amt)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/urllib3/response.py\", line 515, in read\n",
      "    data = self._fp.read() if not fp_closed else b\"\"\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/http/client.py\", line 481, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/http/client.py\", line 630, in _safe_read\n",
      "    data = self.fp.read(amt)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/utils/data_utils.py\", line 647, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/tmp/ipykernel_11396/3441579348.py\", line 72, in __getitem__\n",
      "    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 441, in load\n",
      "    return pickle.load(fid, **pickle_kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/spec.py\", line 1748, in read\n",
      "    out = self.cache._fetch(self.loc, self.loc + length)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/caching.py\", line 397, in _fetch\n",
      "    new = self.fetcher(self.end, bend)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/s3fs/core.py\", line 1200, in _fetch_range\n",
      "    return _fetch_range(self.fs.s3, self.bucket, self.key, self.version_id, start, end, req_kw=self.req_kw)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/s3fs/core.py\", line 1333, in _fetch_range\n",
      "    resp = client.get_object(Bucket=bucket, Key=key,\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py\", line 530, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py\", line 947, in _make_api_call\n",
      "    http, parsed_response = self._make_request(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py\", line 970, in _make_request\n",
      "    return self._endpoint.make_request(operation_model, request_dict)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/endpoint.py\", line 119, in make_request\n",
      "    return self._send_request(request_dict, operation_model)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/endpoint.py\", line 199, in _send_request\n",
      "    success_response, exception = self._get_response(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/endpoint.py\", line 241, in _get_response\n",
      "    success_response, exception = self._do_get_response(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/endpoint.py\", line 281, in _do_get_response\n",
      "    http_response = self._send(request)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/endpoint.py\", line 377, in _send\n",
      "    return self.http_session.send(request)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/httpsession.py\", line 465, in send\n",
      "    urllib_response = conn.urlopen(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/http/client.py\", line 1374, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./trained_model.ckpt\"\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "tf.keras.backend.set_image_data_format(\"channels_last\")\n",
    "\n",
    "train_data_gen = DataGenerator(\"s3://training-data-lstm/processed_data_1/\", 32, True, \"train\")\n",
    "val_data_gen = DataGenerator(\"s3://training-data-lstm/processed_data_1/\", 32, True, \"train\")\n",
    "\n",
    "num_embedding_features = 14\n",
    "num_historical_features = 14\n",
    "historical_data_window = 14\n",
    "future_prediction_window = 3\n",
    "\n",
    "model = define_model(num_embedding_features, num_historical_features, historical_data_window, future_prediction_window)\n",
    "history = train_model(epochs, batch_size, model_path, model, train_data_gen, val_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "45041760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 14, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (None, 14, 32)       6016        ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           480         ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 14, 32)       0           ['lstm_10[1][0]']                \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 32)           0           ['dense_8[1][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 14, 32)      128         ['re_lu_17[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32)          128         ['re_lu_20[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)                 (None, 14, 64)       24832       ['batch_normalization_10[1][0]'] \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64)           2112        ['batch_normalization_12[1][0]'] \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 14, 64)       0           ['lstm_11[1][0]']                \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 64)           0           ['dense_9[1][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 14, 64)      256         ['re_lu_18[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 64)          256         ['re_lu_21[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)                 (None, 32)           12416       ['batch_normalization_11[1][0]'] \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 32)           2080        ['batch_normalization_13[1][0]'] \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 32)           0           ['lstm_12[1][0]']                \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 32)           0           ['dense_10[1][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 64)           0           ['re_lu_19[1][0]',               \n",
      "                                                                  're_lu_22[1][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 32)           2080        ['concatenate_3[1][0]']          \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 32)           0           ['dense_11[1][0]']               \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVector)  (None, 3, 32)       0           ['re_lu_23[1][0]']               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 3, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 3, 33)        0           ['repeat_vector_1[1][0]',        \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)                 (None, 3, 32)        8448        ['concatenate_4[1][0]']          \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 3, 32)        0           ['lstm_13[1][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 3, 32)       128         ['re_lu_24[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 (None, 3, 16)        3136        ['batch_normalization_14[1][0]'] \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 3, 16)        0           ['lstm_14[1][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 3, 16)       64          ['re_lu_25[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_15 (LSTM)                 (None, 3, 2)         152         ['batch_normalization_15[1][0]'] \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 3, 2)         0           ['lstm_15[1][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62,712\n",
      "Trainable params: 62,232\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff49600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.file_types = [\"train_frames_x\", \"train_frames_y\", \"train_frames_embed\", \"train_frames_decoder_input\"]\n",
    "        self.indices = []\n",
    "        for i in range(1, 2):\n",
    "            for j in range(1, 2):\n",
    "                self.indices += [f\"_{i}_{j}\"]\n",
    "        self.on_epoch_end()\n",
    "        self.total_samples = 0\n",
    "        self.num_samples_per_index = {}\n",
    "        for index in self.indices:\n",
    "            data = np.load(self.file_types[3] + index + \".pkl\", allow_pickle=True)\n",
    "            self.total_samples += data.shape[0]\n",
    "            self.num_samples_per_index[index] = data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.total_samples/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        lower_idx = index * self.batch_size\n",
    "        upper_idx = (index+1) * self.batch_size - 1\n",
    "        \n",
    "        start = 0\n",
    "        end = 0\n",
    "        begin = 0\n",
    "        finish = 0\n",
    "        starting_idx = \"\"\n",
    "        ending_idx = \"\"\n",
    "        \n",
    "        for idx in self.indices:\n",
    "            end = start + self.num_samples_per_index[idx]\n",
    "            if lower_idx < end and lower_idx >= start:\n",
    "                starting_idx = idx\n",
    "                begin = lower_idx-start\n",
    "            if upper_idx < end and upper_idx >= start:\n",
    "                ending_idx = idx\n",
    "                finish = upper_idx-start\n",
    "                break\n",
    "            start = end\n",
    "        \n",
    "        train_data_x, train_data_embed, train_data_decoder_input, train_data_y = [], [], [], []\n",
    "        started = False\n",
    "        for idx in self.indices:\n",
    "            if not started and idx != starting_idx:\n",
    "                continue\n",
    "            if idx == starting_idx:\n",
    "                if idx == ending_idx:\n",
    "                    train_data_x += [np.load(\"train_frames_x\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(\"train_frames_y\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(\"train_frames_embed\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [np.load(\"train_frames_decoder_input\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    break\n",
    "                else:\n",
    "                    train_data_x += [np.load(\"train_frames_x\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(\"train_frames_y\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(\"train_frames_embed\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [np.load(\"train_frames_decoder_input\"+idx+\".pkl\", allow_pickle=True)[begin:finish+1]]\n",
    "            elif idx != ending_idx:\n",
    "                train_data_x += [np.load(\"train_frames_x\"+idx+\".pkl\", allow_pickle=True)]\n",
    "                train_data_y += [np.load(\"train_frames_y\"+idx+\".pkl\", allow_pickle=True)]\n",
    "                train_data_embed += [np.load(\"train_frames_embed\"+idx+\".pkl\", allow_pickle=True)]\n",
    "                train_data_decoder_input += [np.load(\"train_frames_decoder_input\"+idx+\".pkl\", allow_pickle=True)]\n",
    "            elif idx == ending_idx:\n",
    "                train_data_x += [np.load(\"train_frames_x\"+idx+\".pkl\", allow_pickle=True)[:finish+1]]\n",
    "                train_data_y += [np.load(\"train_frames_y\"+idx+\".pkl\", allow_pickle=True)[:finish+1]]\n",
    "                train_data_embed += [np.load(\"train_frames_embed\"+idx+\".pkl\", allow_pickle=True)[:finish+1]]\n",
    "                train_data_decoder_input += [np.load(\"train_frames_decoder_input\"+idx+\".pkl\", allow_pickle=True)[:finish+1]]\n",
    "        \n",
    "        train_data_x, train_data_y, train_data_embed, train_data_decoder_input = pd.concat(train_data_x), pd.concat(train_data_y), pd.concat(train_data_embed), pd.concat(train_data_decoder_input)\n",
    "        \n",
    "        return [train_data_x, train_data_embed, train_data_decoder_input], train_data_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7959ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploaded /home/ec2-user/SageMaker to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/sourcedir.tar.gz\n",
      "Uploaded /home/ec2-user/SageMaker to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/sourcedir.tar.gz\n",
      "Uploaded /home/ec2-user/SageMaker to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:Uploaded /home/ec2-user/SageMaker to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/sourcedir.tar.gz\n",
      "runproc.sh uploaded to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/runproc.sh\n",
      "runproc.sh uploaded to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/runproc.sh\n",
      "runproc.sh uploaded to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/runproc.sh\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-eu-north-1-321097665711/frameworkprocessor-TF-2023-07-12-18-47-38-194/source/runproc.sh\n",
      "Creating processing-job with name frameworkprocessor-TF-2023-07-12-18-47-38-194\n",
      "Creating processing-job with name frameworkprocessor-TF-2023-07-12-18-47-38-194\n",
      "Creating processing-job with name frameworkprocessor-TF-2023-07-12-18-47-38-194\n",
      "INFO:sagemaker:Creating processing-job with name frameworkprocessor-TF-2023-07-12-18-47-38-194\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateProcessingJob operation: The account-level service limit 'ml.t3.2xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m tp \u001b[38;5;241m=\u001b[39m TensorFlowProcessor(\n\u001b[1;32m      2\u001b[0m     framework_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     sagemaker_session \u001b[38;5;241m=\u001b[39m sagemaker_session\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Run the processing job\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ec2-user/SageMaker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mProcessingInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://training-data-lstm/processed_data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/opt/ml/processing/input/data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mProcessingOutput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/opt/ml/processing/output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://training-data-lstm/model_artifacts/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:284\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/processing.py:1756\u001b[0m, in \u001b[0;36mFrameworkProcessor.run\u001b[0;34m(self, code, source_dir, dependencies, git_config, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m   1751\u001b[0m s3_runproc_sh, inputs, job_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pack_and_upload_code(\n\u001b[1;32m   1752\u001b[0m     code, source_dir, dependencies, git_config, job_name, inputs, kms_key\n\u001b[1;32m   1753\u001b[0m )\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# Submit a processing job.\u001b[39;00m\n\u001b[0;32m-> 1756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_runproc_sh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:284\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/processing.py:667\u001b[0m, in \u001b[0;36mScriptProcessor.run\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    657\u001b[0m normalized_inputs, normalized_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_args(\n\u001b[1;32m    658\u001b[0m     job_name\u001b[38;5;241m=\u001b[39mjob_name,\n\u001b[1;32m    659\u001b[0m     arguments\u001b[38;5;241m=\u001b[39marguments,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m     kms_key\u001b[38;5;241m=\u001b[39mkms_key,\n\u001b[1;32m    664\u001b[0m )\n\u001b[1;32m    666\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job \u001b[38;5;241m=\u001b[39m \u001b[43mProcessingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalized_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalized_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/processing.py:901\u001b[0m, in \u001b[0;36mProcessingJob.start_new\u001b[0;34m(cls, processor, inputs, outputs, experiment_config)\u001b[0m\n\u001b[1;32m    898\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, process_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Call sagemaker_session.process using the arguments dictionary.\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocess_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    904\u001b[0m     processor\u001b[38;5;241m.\u001b[39msagemaker_session,\n\u001b[1;32m    905\u001b[0m     processor\u001b[38;5;241m.\u001b[39m_current_job_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     processor\u001b[38;5;241m.\u001b[39moutput_kms_key,\n\u001b[1;32m    909\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:1273\u001b[0m, in \u001b[0;36mSession.process\u001b[0;34m(self, inputs, output_config, job_name, resources, stopping_condition, app_specification, environment, network_config, role_arn, tags, experiment_config)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_processing_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m-> 1273\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:5239\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5224\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5227\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5228\u001b[0m ):\n\u001b[1;32m   5229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5230\u001b[0m \n\u001b[1;32m   5231\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5237\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:1271\u001b[0m, in \u001b[0;36mSession.process.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1269\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating processing-job with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m   1270\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m-> 1271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_processing_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateProcessingJob operation: The account-level service limit 'ml.t3.2xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota."
     ]
    }
   ],
   "source": [
    "tp = TensorFlowProcessor(\n",
    "    framework_version='2.3',\n",
    "    role=role,\n",
    "    instance_type='ml.t3.2xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name='frameworkprocessor-TF',\n",
    "    py_version='py37',\n",
    "    sagemaker_session = sagemaker_session\n",
    ")\n",
    "\n",
    "#Run the processing job\n",
    "tp.run(\n",
    "    code='training.py',\n",
    "    source_dir='/home/ec2-user/SageMaker',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source='s3://training-data-lstm/processed_data/',\n",
    "            destination='/opt/ml/processing/input/data'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination='s3://training-data-lstm/model_artifacts/'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55ecf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, file_path, batch_size=32, shuffle=True):\n",
    "        from s3fs.core import S3FileSystem\n",
    "        self.s3 = S3FileSystem()\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.file_types = [\"train_frames_x\", \"train_frames_y\", \"train_frames_embed\", \"train_frames_decoder_input\"]\n",
    "        self.indices = []\n",
    "        for i in range(1, 2):\n",
    "            for j in range(1, 2):\n",
    "                self.indices += [f\"_{i}_{j}\"]\n",
    "        self.on_epoch_end()\n",
    "        self.total_samples = 0\n",
    "        self.num_samples_per_index = {}\n",
    "        for index in self.indices:\n",
    "            data = np.load(self.s3.open(os.path.join(self.file_path, self.file_types[3] + index + \".pkl\")), allow_pickle=True)\n",
    "            self.total_samples += data.shape[0]\n",
    "            self.num_samples_per_index[index] = data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.total_samples/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        lower_idx = index * self.batch_size\n",
    "        upper_idx = (index+1) * self.batch_size - 1\n",
    "        \n",
    "        start = 0\n",
    "        end = 0\n",
    "        begin = 0\n",
    "        finish = 0\n",
    "        starting_idx = \"\"\n",
    "        ending_idx = \"\"\n",
    "        \n",
    "        for idx in self.indices:\n",
    "            end = start + self.num_samples_per_index[idx]\n",
    "            if lower_idx < end and lower_idx >= start:\n",
    "                starting_idx = idx\n",
    "                begin = lower_idx-start\n",
    "            if upper_idx < end and upper_idx >= start:\n",
    "                ending_idx = idx\n",
    "                finish = upper_idx-start\n",
    "                break\n",
    "            start = end\n",
    "        \n",
    "        train_data_x, train_data_embed, train_data_decoder_input, train_data_y = [], [], [], []\n",
    "        started = False\n",
    "        for idx in self.indices:\n",
    "            if not started and idx != starting_idx:\n",
    "                continue\n",
    "            if idx == starting_idx:\n",
    "                if idx == ending_idx:\n",
    "                    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    break\n",
    "                else:\n",
    "                    train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "                    train_data_decoder_input += [self.s3.open(np.load(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[begin:finish+1]]\n",
    "            elif idx != ending_idx:\n",
    "                train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "                train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)]\n",
    "            elif idx == ending_idx:\n",
    "                train_data_x += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_x\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_y += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_y\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_embed += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_embed\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "                train_data_decoder_input += [np.load(self.s3.open(os.path.join(self.file_path, \"train_frames_decoder_input\"+idx+\".pkl\")), allow_pickle=True)[:finish+1]]\n",
    "        \n",
    "        train_data_x, train_data_y, train_data_embed, train_data_decoder_input = np.concatenate(train_data_x), np.concatenate(train_data_y), np.concatenate(train_data_embed), np.concatenate(train_data_decoder_input)\n",
    "        \n",
    "        return [train_data_x[:, :, 1:], train_data_embed, train_data_decoder_input], train_data_y[:, :, 1:]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48afe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "839660d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = np.load(s3.open(os.path.join(\"s3://training-data-lstm/processed_data_1/\", \"train_frames_embed_1_1\"+\".pkl\")), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44653e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333100, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb5c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(\"s3://training-data-lstm/processed_data_1/\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28843e78",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     91\u001b[0m         train_data_decoder_input \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_frames_decoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:finish\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     93\u001b[0m train_data_x, train_data_y, train_data_embed, train_data_decoder_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(train_data_x), np\u001b[38;5;241m.\u001b[39mconcatenate(train_data_y), np\u001b[38;5;241m.\u001b[39mconcatenate(train_data_embed), np\u001b[38;5;241m.\u001b[39mconcatenate(train_data_decoder_input)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan_to_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), np\u001b[38;5;241m.\u001b[39mnan_to_num(np\u001b[38;5;241m.\u001b[39msqueeze(train_data_embed[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m:]))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), np\u001b[38;5;241m.\u001b[39mnan_to_num(train_data_decoder_input[:, :, \u001b[38;5;241m2\u001b[39m:])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)], np\u001b[38;5;241m.\u001b[39mnan_to_num(train_data_y[:, :, \u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnan_to_num\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/lib/type_check.py:400\u001b[0m, in \u001b[0;36m_nan_to_num_dispatcher\u001b[0;34m(x, copy, nan, posinf, neginf)\u001b[0m\n\u001b[1;32m    396\u001b[0m     f \u001b[38;5;241m=\u001b[39m getlimits\u001b[38;5;241m.\u001b[39mfinfo(t)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mmax, f\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nan_to_num_dispatcher\u001b[39m(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, posinf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, neginf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nan_to_num_dispatcher)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnan_to_num\u001b[39m(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, posinf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, neginf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = train_data_gen.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9a5ed88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 3)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ae4947f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[14261286, 0.0, 0.0],\n",
       "        [14261019, 0.0, 0.0],\n",
       "        [14260726, 0.0, 0.0]],\n",
       "\n",
       "       [[15709598, 0.0, 0.0],\n",
       "        [15708388, 0.0, 0.0],\n",
       "        [15707137, 0.0, 0.0]],\n",
       "\n",
       "       [[15708388, 0.0, 0.0],\n",
       "        [15707137, 0.0, 0.0],\n",
       "        [15705882, 0.0, 0.0]],\n",
       "\n",
       "       [[26532881, 0.0, 0.0],\n",
       "        [26531270, 0.55, 0.0],\n",
       "        [26529737, 0.0, 0.0]],\n",
       "\n",
       "       [[26531270, 0.55, 0.0],\n",
       "        [26529737, 0.0, 0.0],\n",
       "        [26528007, 0.0, 0.0]],\n",
       "\n",
       "       [[26529737, 0.0, 0.0],\n",
       "        [26528007, 0.0, 0.0],\n",
       "        [26526002, 0.49, 0.0]],\n",
       "\n",
       "       [[26528007, 0.0, 0.0],\n",
       "        [26526002, 0.49, 0.0],\n",
       "        [26523999, 0.26, 0.0]],\n",
       "\n",
       "       [[26526002, 0.49, 0.0],\n",
       "        [26523999, 0.26, 0.0],\n",
       "        [26521961, 0.3, 0.0]],\n",
       "\n",
       "       [[26523999, 0.26, 0.0],\n",
       "        [26521961, 0.3, 0.0],\n",
       "        [26520276, 0.74, 0.0]],\n",
       "\n",
       "       [[26521961, 0.3, 0.0],\n",
       "        [26520276, 0.74, 0.0],\n",
       "        [26518934, 0.0, 0.0]],\n",
       "\n",
       "       [[18093645, 0.0, 0.0],\n",
       "        [18093619, 0.0, 0.0],\n",
       "        [18093586, 0.0, 0.0]],\n",
       "\n",
       "       [[18093619, 0.0, 0.0],\n",
       "        [18093586, 0.0, 0.0],\n",
       "        [18093550, 0.0, 0.0]],\n",
       "\n",
       "       [[18093586, 0.0, 0.0],\n",
       "        [18093550, 0.0, 0.0],\n",
       "        [18093514, 0.0, 0.0]],\n",
       "\n",
       "       [[18093550, 0.0, 0.0],\n",
       "        [18093514, 0.0, 0.0],\n",
       "        [18093477, 0.0, 0.0]],\n",
       "\n",
       "       [[18093514, 0.0, 0.0],\n",
       "        [18093477, 0.0, 0.0],\n",
       "        [18093438, 0.0, 0.0]],\n",
       "\n",
       "       [[18093477, 0.0, 0.0],\n",
       "        [18093438, 0.0, 0.0],\n",
       "        [18093398, 0.9122574165636588, 0.0]],\n",
       "\n",
       "       [[18093438, 0.0, 0.0],\n",
       "        [18093398, 0.9122574165636588, 0.0],\n",
       "        [18093354, 0.0, 0.0]],\n",
       "\n",
       "       [[18093398, 0.9122574165636588, 0.0],\n",
       "        [18093354, 0.0, 0.0],\n",
       "        [18093317, 0.0, 0.0]],\n",
       "\n",
       "       [[18093354, 0.0, 0.0],\n",
       "        [18093317, 0.0, 0.0],\n",
       "        [18093280, 0.0, 0.0]],\n",
       "\n",
       "       [[18093317, 0.0, 0.0],\n",
       "        [18093280, 0.0, 0.0],\n",
       "        [18093241, 0.3922234996550931, 16.12730742699471]],\n",
       "\n",
       "       [[18093280, 0.0, 0.0],\n",
       "        [18093241, 0.3922234996550931, 16.12730742699471],\n",
       "        [18093204, 0.0, 0.0]],\n",
       "\n",
       "       [[18093241, 0.3922234996550931, 16.12730742699471],\n",
       "        [18093204, 0.0, 0.0],\n",
       "        [18093171, 0.0, 0.0]],\n",
       "\n",
       "       [[18093204, 0.0, 0.0],\n",
       "        [18093171, 0.0, 0.0],\n",
       "        [18093141, 0.0, 0.0]],\n",
       "\n",
       "       [[18093171, 0.0, 0.0],\n",
       "        [18093141, 0.0, 0.0],\n",
       "        [18093102, 0.0, 0.0]],\n",
       "\n",
       "       [[18093141, 0.0, 0.0],\n",
       "        [18093102, 0.0, 0.0],\n",
       "        [18093066, 0.9432878598566948, 0.0]],\n",
       "\n",
       "       [[18093102, 0.0, 0.0],\n",
       "        [18093066, 0.9432878598566948, 0.0],\n",
       "        [18093027, 0.0, 0.0]],\n",
       "\n",
       "       [[18093066, 0.9432878598566948, 0.0],\n",
       "        [18093027, 0.0, 0.0],\n",
       "        [18092988, 0.0, 0.0]],\n",
       "\n",
       "       [[18093027, 0.0, 0.0],\n",
       "        [18092988, 0.0, 0.0],\n",
       "        [18092935, 0.0, 0.0]],\n",
       "\n",
       "       [[18092988, 0.0, 0.0],\n",
       "        [18092935, 0.0, 0.0],\n",
       "        [18092874, 0.0, 0.0]],\n",
       "\n",
       "       [[18092935, 0.0, 0.0],\n",
       "        [18092874, 0.0, 0.0],\n",
       "        [18092818, 0.0, 0.0]],\n",
       "\n",
       "       [[18092874, 0.0, 0.0],\n",
       "        [18092818, 0.0, 0.0],\n",
       "        [18092752, 0.0, 0.0]],\n",
       "\n",
       "       [[18092818, 0.0, 0.0],\n",
       "        [18092752, 0.0, 0.0],\n",
       "        [18092688, 0.0, 0.0]]], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fde9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1_1\n",
      "x_1_2\n",
      "x_1_3\n",
      "x_1_4\n",
      "x_2_1\n",
      "x_2_2\n",
      "x_2_3\n",
      "x_2_4\n",
      "x_3_1\n",
      "x_3_2\n",
      "x_3_3\n",
      "x_3_4\n",
      "x_4_1\n",
      "x_4_2\n",
      "x_4_3\n",
      "x_4_4\n",
      "x_5_1\n",
      "x_5_2\n",
      "x_5_3\n",
      "x_5_4\n",
      "x_6_1\n",
      "x_6_2\n",
      "x_6_3\n",
      "x_6_4\n",
      "x_7_1\n",
      "x_7_2\n",
      "x_7_3\n",
      "x_7_4\n",
      "x_8_1\n",
      "x_8_2\n",
      "x_8_3\n",
      "x_8_4\n",
      "y_1_1\n",
      "y_1_2\n",
      "y_1_3\n",
      "y_1_4\n",
      "y_2_1\n",
      "y_2_2\n",
      "y_2_3\n",
      "y_2_4\n",
      "y_3_1\n",
      "y_3_2\n",
      "y_3_3\n",
      "y_3_4\n",
      "y_4_1\n",
      "y_4_2\n",
      "y_4_3\n",
      "y_4_4\n",
      "y_5_1\n",
      "y_5_2\n",
      "y_5_3\n",
      "y_5_4\n",
      "y_6_1\n",
      "y_6_2\n",
      "y_6_3\n",
      "y_6_4\n",
      "y_7_1\n",
      "y_7_2\n",
      "y_7_3\n",
      "y_7_4\n",
      "y_8_1\n",
      "y_8_2\n",
      "y_8_3\n",
      "y_8_4\n",
      "embed_1_1\n",
      "embed_1_2\n",
      "embed_1_3\n",
      "embed_1_4\n",
      "embed_2_1\n",
      "embed_2_2\n",
      "embed_2_3\n",
      "embed_2_4\n",
      "embed_3_1\n",
      "embed_3_2\n",
      "embed_3_3\n",
      "embed_3_4\n",
      "embed_4_1\n",
      "embed_4_2\n",
      "embed_4_3\n",
      "embed_4_4\n",
      "embed_5_1\n",
      "embed_5_2\n",
      "embed_5_3\n",
      "embed_5_4\n",
      "embed_6_1\n",
      "embed_6_2\n",
      "embed_6_3\n",
      "embed_6_4\n",
      "embed_7_1\n",
      "embed_7_2\n",
      "embed_7_3\n",
      "embed_7_4\n",
      "embed_8_1\n",
      "embed_8_2\n",
      "embed_8_3\n",
      "embed_8_4\n",
      "decoder_input_1_1\n",
      "decoder_input_1_2\n",
      "decoder_input_1_3\n",
      "decoder_input_1_4\n",
      "decoder_input_2_1\n",
      "decoder_input_2_2\n",
      "decoder_input_2_3\n",
      "decoder_input_2_4\n",
      "decoder_input_3_1\n",
      "decoder_input_3_2\n",
      "decoder_input_3_3\n",
      "decoder_input_3_4\n",
      "decoder_input_4_1\n",
      "decoder_input_4_2\n",
      "decoder_input_4_3\n",
      "decoder_input_4_4\n",
      "decoder_input_5_1\n",
      "decoder_input_5_2\n",
      "decoder_input_5_3\n",
      "decoder_input_5_4\n",
      "decoder_input_6_1\n",
      "decoder_input_6_2\n",
      "decoder_input_6_3\n",
      "decoder_input_6_4\n",
      "decoder_input_7_1\n",
      "decoder_input_7_2\n",
      "decoder_input_7_3\n",
      "decoder_input_7_4\n",
      "decoder_input_8_1\n",
      "decoder_input_8_2\n",
      "decoder_input_8_3\n",
      "decoder_input_8_4\n"
     ]
    }
   ],
   "source": [
    "from s3fs.core import S3FileSystem\n",
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "import pickle\n",
    "s3 = S3FileSystem()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "file_types = [\"x\", \"y\", \"embed\", \"decoder_input\"]\n",
    "for file_type in file_types:\n",
    "    for i in range(1, 9):\n",
    "        for j in range(1, 5):\n",
    "            print(f\"{file_type}_{i}_{j}\")\n",
    "            test_arr = np.load(s3.open(os.path.join(\"s3://training-data-lstm/processed_data/\", f\"test_frames_{file_type}_{i}_{j}.pkl\")), allow_pickle=True)\n",
    "            test_arr1 = np.concatenate([sample for sample in test_arr], axis=0)\n",
    "            my_array_data = io.BytesIO()\n",
    "            pickle.dump(test_arr1, my_array_data)\n",
    "            my_array_data.seek(0)\n",
    "            s3_client.upload_fileobj(my_array_data, 'training-data-lstm', f'processed_data_1/test_frames_{file_type}_{i}_{j}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1fea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs.core import S3FileSystem\n",
    "s3 = S3FileSystem()\n",
    "import numpy as np\n",
    "import os\n",
    "test_arr = np.load(s3.open(os.path.join(\"s3://training-data-lstm/processed_data/\", \"train_frames_y_1_1\"+\".pkl\")), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3d169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9420,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520f18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr1 = np.concatenate([sample for sample in test_arr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfbc388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333100, 3, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "087cbffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 14, 15)\n",
      "(2, 14, 15)\n",
      "(28, 14, 15)\n",
      "(21, 14, 15)\n",
      "(3, 14, 15)\n",
      "(20, 14, 15)\n",
      "(7, 14, 15)\n",
      "(3, 14, 15)\n",
      "(17, 14, 15)\n",
      "(23, 14, 15)\n",
      "(28, 14, 15)\n",
      "(12, 14, 15)\n",
      "(2, 14, 15)\n",
      "(24, 14, 15)\n",
      "(8, 14, 15)\n",
      "(76, 14, 15)\n",
      "(8, 14, 15)\n",
      "(35, 14, 15)\n",
      "(16, 14, 15)\n",
      "(9, 14, 15)\n",
      "(14, 14, 15)\n",
      "(10, 14, 15)\n",
      "(6, 14, 15)\n",
      "(32, 14, 15)\n",
      "(37, 14, 15)\n",
      "(86, 14, 15)\n",
      "(112, 14, 15)\n",
      "(6, 14, 15)\n",
      "(4, 14, 15)\n",
      "(183, 14, 15)\n",
      "(49, 14, 15)\n",
      "(130, 14, 15)\n"
     ]
    }
   ],
   "source": [
    "for sample in X[0]:\n",
    "    print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab7909b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 3, 3)\n",
      "(2, 3, 3)\n",
      "(28, 3, 3)\n",
      "(21, 3, 3)\n",
      "(3, 3, 3)\n",
      "(20, 3, 3)\n",
      "(7, 3, 3)\n",
      "(3, 3, 3)\n",
      "(17, 3, 3)\n",
      "(23, 3, 3)\n",
      "(28, 3, 3)\n",
      "(12, 3, 3)\n",
      "(2, 3, 3)\n",
      "(24, 3, 3)\n",
      "(8, 3, 3)\n",
      "(76, 3, 3)\n",
      "(8, 3, 3)\n",
      "(35, 3, 3)\n",
      "(16, 3, 3)\n",
      "(9, 3, 3)\n",
      "(14, 3, 3)\n",
      "(10, 3, 3)\n",
      "(6, 3, 3)\n",
      "(32, 3, 3)\n",
      "(37, 3, 3)\n",
      "(86, 3, 3)\n",
      "(112, 3, 3)\n",
      "(6, 3, 3)\n",
      "(4, 3, 3)\n",
      "(183, 3, 3)\n",
      "(49, 3, 3)\n",
      "(130, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "for sample in X[2]:\n",
    "    print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18f45a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 's3://training-data-lstm/processed_data/train_frames_x_1_1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://training-data-lstm/processed_data/train_frames_x_1_1.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 's3://training-data-lstm/processed_data/train_frames_x_1_1.pkl'"
     ]
    }
   ],
   "source": [
    "np.load(\"s3://training-data-lstm/processed_data/train_frames_x_1_1.pkl\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d024879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"s3://training-data-lstm/keyword_level_complete_data2 (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04428a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
